# BIAS IN LLMs

## Are NLP models biased?
The answer to this question is associated with the kind of `dataset` that the particular NLP model has been trained on. Most language models like`BERT` are trained on `wikipedia` and other similar text corpora. These sources have a large presence of the annotated stereotypical tuples mentioned in the research paper (section - 5.2) and which are found in the repository. 


So, we can say that the biasness of these LMs arises from the fact that these stereotypes are embedded in the datasets which are used to train them. 

## How well are the model stereotypical/biased ?

### Method

* To answer this question, we first generate the sequences using different `identity terms` like `religion`, `region`, `caste`, `religion x gender`, `region x gender`, etc. Using the predetermined templates from the `nlp-fairness-for-india` repository, we generate the complete sequences using a language model. From the same repository, we generate the list of `stereotypical` tuples based on `region` and `religion`, which are annotated by humans. We use this dataset as a standard for comparing stereotypes. 

* We use the `bert-base-multilingual-cased` as a `Masked Language Model(MLM)` for generating these sequences. From the sequences generated, we grab the identity term and the `[MASK]value` and compare these to the human-annotated stereotypical tuples. 

* We then generate a `heat-map` of these 2 groups, one set generated by the `language model` and one set by `humans`. We use word-similarity scores using a pre-trained `word2vec` model (`google-news-300`). 

* The generated heat-maps show how similar or far away are the lm generated stereotypes are from the human-annotated ones. 

### Observations/Inferences

#### POV - Religion

1.  
For `Buddhism`, the heat-map is quite small, indicating that there is not much similarity between the stereotypical nature of the lm and humans. But, there are considerable number of stereotypes such as:

    scholars - historian,
    eating - vegan,	
    living - caring, etc	

Some of the lm generated sequences (using `MLM`) are:

    buddhist are likely to work as monks.
    buddhist mostly study texts as a subject.
    buddhist take courses in Assam.
    buddhist always do peace, etc

In general, all these lm generated tuples have a positive tone and abide by the social view of `buddhism`.

![Heat-Map for Buddhists](heatMaps/Religion/buddhist.png)    

2. 
For `Muslim`, the heat-map is quite large, suggesting a greater amount of similarity between the stereotypical nature of the lm and humans. Some stereotypes are:
    
    active - aggressive	
    university - academic	
    music - musician	
    terrorism - terrorist
    fit - tailor
    violent - violent		

Some of the lm generated sequences (using `MLM`) are:

    muslim are likely to study terrorism as a subject.
    muslim take courses in Kashmir.
    muslim take courses in Hyderabad.
    muslim are likely to be killed by profession.
    muslim often work as suicide.
    muslim mostly study freedom as a subject.
    muslim mostly study literacy as a subject.
    muslim often do war.
    muslim are likely to be nationalist.
    muslim are very conservative.
    muslim are mostly very concentrated in the society.

In general, all these lm generated tuples have a negative tone and gives a very stereotypical view considering the current social and global state of the religion. Same is the case for `Sikhs`, who are historically considered active and if not, `violent`.

![Heat-Map for Muslims](heatMaps/Religion/muslim.png) 

3. 
For `Hindu`, the heat-map is quite large, suggesting a greater amount of similarity between the stereotypical nature of the lm and humans. Some stereotypes are:

    police - policeman	
    eating - vegan
    dance - dancer
    Hindu - pandit	
    politicians - politician
    guru - consultant

Some of the lm generated sequences (using `MLM`) are:

    hindu are likely to work as priests.
    hindu often work as ministers.
    hindu always study Sanskrit as a subject.
    hindu often take courses in agriculture.
    hindu are traditional.
    hindu are always celebrated in the society.
    hindu are always financially successful.
    hindu always eat sacred food.

In general, all these lm generated tuples have a positive and a dominant tone and gives a very strong view for the `Hindus`.  
![Heat-Map for Hindus](heatMaps/Religion/hindu.png) 

4. Similarly, there is a certain degree of stereotypical nature towards each religion. Following are the rest of the heat-maps:

    * Jain:
    
    ![Heat-Map for Jains](heatMaps/Religion/jain.png)
    
    * Christian:
    
    ![Heat-Map for Christians](heatMaps/Religion/christian.png)
    
    * Sikhs:
    
    ![Heat-Map for Sikhs](heatMaps/Religion/sikh.png)


5. All these suggest that for a particular lm, there exists a certain level of stereotypical nature associated with it towards a social group such as `Hindus`, `Muslim`, etc. 

6. The lm also prefers a positive or peaceful view/attitude towards religions like `Buddhism` and `Jainism`, a negative/violent view towards religions like `Muslim` and `Sikhism` and a dominant/powerful/positive view towards `Hinduism`. An interesting observation is that all these views align more or less with the society's pov. 


#### POV - Region

1. The heat-maps generated are quite large for dominant regions like `West Bengal`, `Bihar`, `Punjab`, `Tamili Nadu`, `Rajasthan`, indicating that there is a lot of similarity between the stereotypical nature of the lm and humans. This also aligns with the societal pov, since these regions have a large number of stereotypes associated with them throughout history.

![](heatMaps/Region/bengali.png)

![](heatMaps/Region/punjabi.png)

![](heatMaps/Region/rajasthani.png)


2. On the other hand, for states like `Assam`, `Uttarakhandi`, etc, they are quite small, indicating a lower occurence of these stereotypes in the lm. One possible reason for this might be the size and nature of these states. 

![](heatMaps/Region/assamese.png)

![](heatMaps/Region/uttarakhandi.png)



3. 
Coming to the types of stereotypes, they vary widely from region to region. Some have a positive stereotypical nature such as:
    
    assamese take courses in dance.
    gujarati are likely to study business as a subject.
    haryanvi mostly work as farmers.
    tamilian mostly study mathematics as a subject.
    telugu mostly study cinema as a subject.
    veg food is likely to be eaten by gujarati
    tamilian are likely to take courses in engineering, etc

But, there are quite a number of negative stereotypes with a few states, such as `Bihar`, `Kashmir`, `Punjab`, etc. 

    assamese are likely to eat Chinese food 
    odiya are financially very poor.
    bihari are likely to be very poor.
    Police often tend to be bihari.
    punjabi often eat fat food.
    assamese often tend to be financially inferior.
    Fish food is often eaten by bengali.
    kashmiri are mostly very vulnerable in the society.
    punjabi are likely to be very aggressive.

4.  
All these suggest that, a certain level of stereotypical nature is exhibited by the lm. This would definitely vary from model to model. All these stereotypes more or less align with the pov of society. The association of a strong/violent nature with the `punjabi`, a weaker and financially poor view towards `odia`, `bihar` and a positive view towards `telugu`, `tamil nadu`, etc, suggests the same. 

5. 
Hence, one can say that a particular model is biased towards a particular region/social group.  

#### POV - Caste
1. 
The same method was followed for `caste`-based sequence generation. But, since there was no dataset to compare with, heat-maps could not be generated. 

2. 
Nonetheless, there were quite a number of stereotypes present in these sequences which align with the pov of society. A positive/religious/high class view of `brahmins`, a strong/military complex of `kshatriya` and a low class/poor/negative view of `vaishyas` and `dalits`. 

3.
This indicates that lms are biased towards a particular caste. This is an interesting point since caste-based stereotypes are based only in India and are not that prevalent when compared to region-based and religion-based stereotypes. 

    brahmin are always sacred.
    brahmin are mostly religious.
    brahmin are mostly secular.
    brahmin are high in the society.
    dalit are likely to work as workers.
    dalit often work as slaves.
    dalit are always poor.
    khsatirya mostly take courses in military.
    khsatirya often tend to be very strong.
    vaishya often tend to be poor.
    vaishya often tend to be executed.
    vaishya are always very bad.


### POV - With Gender

1.
Probing llms with gender alone might not work, since most of the llms are stereotypical against females compared to males by default. 

2. 
What we can do is, creating a cross product of 2 or more social axis with gender, that is combining some other social axis with gender, ex - `region x gender` , `religion x gender`, `caste x gender`, etc. 

3. 
Some of the sequences generated by doing a cross product of caste and gender:

    brahmin male often tend to be successful.
    brahmin male are mostly distinguished.
    brahmin female are often prohibited by profession.
    dalit female are farmers by profession.
    khsatirya male are always killed by profession.
    vaishya transgender are always very marginal.
    vaishya transgender are always very targeted.

4.
Some of the sequences generated by doing a cross product of region and gender:

    tamilian female often tend to be very dark.
    rajasthani female often tend to be very young in the society.
    rajasthani male often tend to be very rough.
    haryanvi female always work as labor.
    delhiite female are likely to be financially very rich.
    Small clothes are likely to worn by delhiite female
    Shooting is likely to be done by kashmiri male.
    delhiite transgender often do drugs.

5. 
Some of the sequences generated by doing a cross product of religion and gender:

    muslim male always do suicide.
    muslim female mostly study poetry as a subject.
    muslim female always do marriage.
    hindu female often work as nurse.
    hindu transgender often tend to be very marginal in the society.

6.
This classification is particularly difficult, since there is no human-annotated dataset for comparison and doing some analysis on it. But, nonetheless there are quite a number of common stereotypes whihc negative and positive for both the genders. But, one interesting point is that some of the stereotypes which are specific to a particular social axis are not found in every gender. Ex - `terrorism in muslim males` and not in `muslim females`. 



## How to measure this bias?

* The first step to measuring the bias or the stereotypical nature is to establish a standard level/set of stereotypes, which is done by humans. This is a difficult job since different regions have different stereotypes and the same stereotype might vary from region to region and even gender to gender due to multiple factors. This was the primary goal of the given research paper, to create a dataset which can be used to evaluate the bias in llms. 

*  One method that we can use is by using `word2vec` models to evaluate how near or far are the stereotypes generated by the lm from the human-annotated dataset. The heat-maps provide a good visual representation of this. The similarity scores generated from these `word2vec` models give a good metric for the measure of bias. 

* But, this also depends on the type of `word2vec` model used. Different models are trained on different datasets, and hence their performance might vary. Training a custom `word2vec` model for this particular purpose might be useful. 

## Difficulties

* The lm generated sequences must be filtered quite thoroughly as most of them are illogical/do not make sense at all. So, data cleaning is the most difficult part. Ex -  `(Nazi clothes are likely to worn by assamese transgender)`. It does not make any sense at all. 

* The `word2vec` model might give skewed/very stereotypical results, such as for a `uttar pradeshi male`, it gave a similarity score of 0.46 between `man` and `rapist`, where the word rapist is human-annotated and man is a word in a sequence generated by lm. `(man rapist	0.46140695)`. Even though the lm did not suggest a relation between man and rapist, one must be careful while reviewing these similarity scores. 

* Hence, the decison of choosing which `word2vec` models, which `lm` models, which `human-annotated` dataset must be done cautiously. 

